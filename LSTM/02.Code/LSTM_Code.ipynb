{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# plotting\n",
    "import matplotlib as mpl\n",
    "mpl.style.use('ggplot')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# math and data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# to handle paths\n",
    "from pathlib import Path\n",
    "\n",
    "# set random seeds \n",
    "from numpy.random import seed\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "# to handle warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# to handle pre-processing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# modeling\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "# progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# to save model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "RANDOM_SEED = 2018\n",
    "seed(RANDOM_SEED)\n",
    "set_random_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create dummies for given cols set\n",
    "\n",
    "def create_dummies(data, cols):\n",
    "    data_dummies = pd.get_dummies(data[cols])\n",
    "    data = pd.concat([data,data_dummies],axis=1)\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data pre-processing steps use for both train and test dataset\n",
    "def data_preprocessing(data, cols=[]):\n",
    "    data['date'] = data.timestamp.dt.date\n",
    "    data['weekday'] = data.timestamp.dt.weekday\n",
    "    cols = ['series_id','date','weekday'] + cols\n",
    "    data = data.groupby(cols)['consumption'].sum().reset_index()\n",
    "    data['timestamp'] = data['date']\n",
    "    data =create_dummies(data, \"weekday\")\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "\n",
    "def prepare_training_data(consumption_series, n_lags,n_features):\n",
    "    \"\"\" Converts a series of consumption data into a\n",
    "        lagged, scaled sample.\n",
    "    \"\"\"\n",
    "    n_obs = n_lags*n_features\n",
    "    # scale training data\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    #scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled = scaler.fit_transform(consumption_series)\n",
    "    #consumption_vals = scaler.fit_transform(consumption_series.values.reshape(-1, 1))\n",
    "    #consumption_vals = scaled.values.reshape(-1, 1)\n",
    "    \n",
    "    \n",
    "    consumption_lagged1 = series_to_supervised(scaled, n_lags, 1)\n",
    "    consumption_lagged = consumption_lagged1.values\n",
    "    \n",
    "    \n",
    "    X, y = consumption_lagged[:, :n_obs], consumption_lagged[:, -n_features]\n",
    "    X = X.reshape((X.shape[0], n_lags, n_features))\n",
    "    \n",
    "    return X, y, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update X to be latest data plus prediction\n",
    "def generate_features_prediction(pred, X, lag, scaler):\n",
    "    if (lag>1):\n",
    "        X = X[no_features:]\n",
    "        last_entry = list(X[-(no_features-1):])\n",
    "    else:\n",
    "        last_entry = list(X[1:])\n",
    "        X = []\n",
    "        \n",
    "    weekday_ind = last_entry.index(1)\n",
    "    weekday_ind = (weekday_ind+1)%7\n",
    "\n",
    "    weekday_features = no_features*[-1]\n",
    "    weekday_features[weekday_ind] = 1\n",
    "    \n",
    "    X = list(X)\n",
    "    X.append(pred)\n",
    "\n",
    "    for j in range(no_features-1):\n",
    "        X.append(weekday_features[j])\n",
    "    X = np.array(X)\n",
    "    \n",
    "    # revert scale back to original range\n",
    "    a1 = no_features*[0]\n",
    "    a1[0] = pred\n",
    "    a1 = np.array(a1)\n",
    "    pred_scaled = (scaler.inverse_transform(a1.reshape(1,-1)))[0][0]\n",
    "   \n",
    "    return X,pred_scaled\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_forecast(num_pred_days, shipment_data, model, scaler, lag, no_features, scaling = True, output_scaled=True):\n",
    "    \"\"\" Uses last hour's prediction to generate next for num_pred_hours, \n",
    "        initialized by most recent cold start prediction. Inverts scale of \n",
    "        predictions before return.\n",
    "    \"\"\"\n",
    "    # allocate prediction frame\n",
    "    preds_scaled = np.zeros(num_pred_days)\n",
    "    n = lag*no_features\n",
    "    \n",
    "    # initial X is last lag values from the cold start\n",
    "    if (scaling):\n",
    "        X = scaler.transform(shipment_data.values.ravel().reshape(-1, 1))[-n:]\n",
    "    else:\n",
    "        X = shipment_data.ravel()[-n:]\n",
    "       \n",
    "    for i in range(num_pred_days):\n",
    "        # predict scaled value for next time step\n",
    "        yhat = model.predict(X.reshape(1, lag,no_features), batch_size=1)[0][0]\n",
    "        \n",
    "        X,yhat_scaled = generate_features_prediction(yhat, X, lag, scaler)\n",
    "        \n",
    "        # revert scale back to original range\n",
    "        if (output_scaled):\n",
    "            preds_scaled[i] = yhat_scaled\n",
    "        else:\n",
    "            preds_scaled[i] = yhat\n",
    "   \n",
    "    return preds_scaled "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary & Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Dictionary\n",
    "data_path = Path('..', '01.Data')\n",
    "\n",
    "### Features\n",
    "cols_to_use = ['consumption', 0,1,2,3,4,5,6]\n",
    "no_features = len(cols_to_use)\n",
    "\n",
    "\n",
    "# model parameters\n",
    "num_neurons = 8\n",
    "lag = 1\n",
    "num_passes_through_data = 10\n",
    "\n",
    "# model store\n",
    "model_path = Path('..', '05.Model')\n",
    "model_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "model_file_name = \"model_02_Test_LSTM_Daily_Weekly-Multivariate-Lag_7_1-V2.h5\"\n",
    "existing_model_load = False\n",
    "existing_model_file_name = \"model_02_Test_LSTM_Daily_Weekly-Multivariate-Lag_7_1-V2.h5\"\n",
    "\n",
    "# result store\n",
    "save_path = Path('..', '06.Test')\n",
    "save_path.mkdir(exist_ok=True, parents=True)\n",
    "result_file_name = \"Lstm_check.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consumption dataset --> (21224, 12)\n",
      "cold start train dataset --> (4666, 12)\n",
      "submission format --> (1963, 13)\n"
     ]
    }
   ],
   "source": [
    "### Consumption data\n",
    "\n",
    "consumption_train = pd.read_csv(data_path / 'consumption_train.csv', index_col=0, parse_dates=['timestamp'])\n",
    "consumption_train = data_preprocessing(consumption_train)\n",
    "print (\"consumption dataset -->\", consumption_train.shape)\n",
    "\n",
    "cold_start_test = pd.read_csv(data_path / 'cold_start_test.csv', index_col=0, parse_dates=['timestamp'])\n",
    "cold_start_test = data_preprocessing(cold_start_test)\n",
    "print (\"cold start train dataset -->\", cold_start_test.shape)\n",
    "\n",
    "submission_format = pd.read_csv(data_path / 'submission_format.csv',index_col='pred_id',parse_dates=['timestamp'])\n",
    "submission_format = data_preprocessing(submission_format,cols = [\"prediction_window\"])\n",
    "print (\"submission format -->\", submission_format.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pre-processing steps\n",
    "\n",
    "cold_start_weekday_data = cold_start_test.groupby(['series_id'])[0,1,2,3,4,5,6].sum().reset_index()\n",
    "cold_start_weekday_data['weekday'] = cold_start_weekday_data[0] + cold_start_weekday_data[1] +cold_start_weekday_data[2]+cold_start_weekday_data[3] +cold_start_weekday_data[4]\n",
    "    \n",
    "cold_start_weekday_data['weekend'] = cold_start_weekday_data[5] + cold_start_weekday_data[6]\n",
    "\n",
    "cold_start_weekday_list = cold_start_weekday_data[(cold_start_weekday_data.weekday==0)]\n",
    "cold_start_weekend_list = cold_start_weekday_data[(cold_start_weekday_data.weekend==0)]\n",
    "\n",
    "cold_start_weekday_missing_list = list(cold_start_weekday_list.series_id)\n",
    "cold_start_weekend_missing_list = list(cold_start_weekend_list.series_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission format--> (1721, 13)\n",
      "output format--> (1721, 13)\n"
     ]
    }
   ],
   "source": [
    "submission_format_filter_daily_weekly = submission_format[submission_format.prediction_window != \"hourly\"]\n",
    "print (\"submission format-->\", submission_format_filter_daily_weekly.shape)\n",
    "\n",
    "# copy submission format and fill in values\n",
    "my_submission = submission_format_filter_daily_weekly.copy()\n",
    "print (\"output format-->\", my_submission.shape)\n",
    "my_submission['consumption'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Learning Consumption Trends - Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 10.7 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning Consumption Trends - Epoch: 100%|██████████| 10/10 [06:12<00:00, 37.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model initiate \n",
    "\n",
    "if (existing_model_load):\n",
    "    json_file = open('model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    model.load_weights(model_path /existing_model_file_name)\n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    print(\"Loaded model from disk\")\n",
    "    \n",
    "else:\n",
    "    \n",
    "    batch_size = 1  # this forces the lstm to step through each time-step one at a time\n",
    "    batch_input_shape=(batch_size, lag, no_features)\n",
    "\n",
    "    # instantiate a sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # add LSTM layer - stateful MUST be true here in \n",
    "    # order to learn the patterns within a series\n",
    "    model.add(LSTM(units=num_neurons, \n",
    "                  batch_input_shape=batch_input_shape, \n",
    "                  stateful=True))\n",
    "\n",
    "    model.add(Dense(3, activation='relu'))\n",
    "    # followed by a dense layer with a single output for regression\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # compile\n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    \n",
    "    \n",
    "    %%time\n",
    "    num_training_series = consumption_train.series_id.nunique()\n",
    "\n",
    "\n",
    "    for i in tqdm(range(num_passes_through_data), \n",
    "                  total=num_passes_through_data, \n",
    "                  desc='Learning Consumption Trends - Epoch'):\n",
    "\n",
    "        # reset the LSTM state for training on each series\n",
    "        for ser_id, ser_data in consumption_train.groupby('series_id'):\n",
    "\n",
    "            # prepare the data\n",
    "            X, y, scaler = prepare_training_data(ser_data[cols_to_use], lag, no_features)\n",
    "\n",
    "            # fit the model: note that we don't shuffle batches (it would ruin the sequence)\n",
    "            # and that we reset states only after an entire X has been fit, instead of after\n",
    "            # each (size 1) batch, as is the case when stateful=False\n",
    "            model.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "            model.reset_states()\n",
    "            \n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "    model_file_name\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(model_path /model_file_name)\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forecasting from Cold Start Data: 100%|██████████| 383/383 [00:11<00:00, 34.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.2 s, sys: 3.89 s, total: 26.1 s\n",
      "Wall time: 11.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pred_window_to_num_preds = {'hourly': 1, 'daily': 7, 'weekly': 14}\n",
    "pred_window_to_num_days = {'hourly': 1, 'daily': 7, 'weekly': 2}\n",
    "#pred_window_to_num_pred_hours = {'hourly': 24, 'daily': 7 * 24, 'weekly': 2 * 7 * 24}\n",
    "\n",
    "num_test_series = my_submission.series_id.nunique()\n",
    "\n",
    "model.reset_states()\n",
    "count=0\n",
    "for ser_id, pred_df in tqdm(my_submission.groupby('series_id'), \n",
    "                            total=num_test_series, \n",
    "                            desc=\"Forecasting from Cold Start Data\"):\n",
    "        \n",
    "    # get info about this series' prediction window\n",
    "    pred_window = pred_df.prediction_window.unique()[0]\n",
    "    num_preds = pred_window_to_num_preds[pred_window]\n",
    "    num_days = pred_window_to_num_days[pred_window]\n",
    "    #num_pred_hours = pred_window_to_num_pred_hours[pred_window]\n",
    "    \n",
    "    # prepare cold start data\n",
    "    series_data = cold_start_test[cold_start_test.series_id == ser_id]\n",
    "    series_data = series_data[cols_to_use]\n",
    "    cold_X, cold_y, scaler = prepare_training_data(series_data, lag, no_features)\n",
    "    if (len(cold_X)):\n",
    "        check_flag = 1\n",
    "    else:\n",
    "        check_flag=0\n",
    "    check_flag = check_flag & (ser_id not in cold_start_weekday_missing_list) & (ser_id not in cold_start_weekend_missing_list)\n",
    "    if (check_flag):\n",
    "        count+=1\n",
    "\n",
    "        # fine tune our lstm model to this site using cold start data    \n",
    "        model.fit(cold_X, cold_y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "        b = series_data[-lag:].values[0]\n",
    "        c = scaler.transform(b.reshape(1,no_features))[0]\n",
    "        preds = generate_forecast(num_preds, c, model, scaler, lag=lag, no_features=no_features, scaling = False)\n",
    "    else:\n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        max_value = max(series_data.consumption)\n",
    "        series_data = series_data.tail(1)\n",
    "        \n",
    "        if ser_id in cold_start_weekday_missing_list:\n",
    "            l1 = no_features*[0]\n",
    "            l1[0] = 1.3*max_value\n",
    "            l1 = pd.DataFrame(data=[l1], columns = series_data.columns)\n",
    "            a = pd.concat([series_data,l1], axis=0)\n",
    "            b = scaler.fit_transform(a)\n",
    "            scaled = scaler.transform(series_data)\n",
    "        elif ser_id in cold_start_weekend_missing_list:\n",
    "            l1 = no_features*[0]\n",
    "            l1[0] = 0.7*max_value\n",
    "            l1 = pd.DataFrame(data=[l1], columns = series_data.columns)\n",
    "            a = pd.concat([series_data,l1], axis=0)\n",
    "            b = scaler.fit_transform(a)\n",
    "            scaled = scaler.transform(series_data)\n",
    "        else:\n",
    "            print (ser_id)\n",
    "            scaled = scaler.fit_transform(series_data)\n",
    "        weekday_ind = list(series_data.values[0]).index(1)\n",
    "        scaled[0][weekday_ind] =1\n",
    "        \n",
    "        # make hourly forecasts for duration of pred window\n",
    "        preds = generate_forecast(num_preds,scaled, model, scaler, lag=lag, no_features=no_features, scaling = False)\n",
    "        \n",
    "        # reduce by taking sum over each sub window in pred window\n",
    "    reduced_preds = [pred.sum() for pred in np.split(preds, num_days)]\n",
    "\n",
    "        # store result in submission DataFrame\n",
    "    ser_id_mask = my_submission.series_id == ser_id\n",
    "    my_submission.loc[ser_id_mask, 'consumption'] = reduced_preds\n",
    "    model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (1, 8)                    544       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, 3)                    27        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (1, 1)                    4         \n",
      "=================================================================\n",
      "Total params: 575\n",
      "Trainable params: 575\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission Done\n"
     ]
    }
   ],
   "source": [
    "save_path = Path('..', '06.Test')\n",
    "\n",
    "\n",
    "my_submission_final  = my_submission[(my_submission.consumption != 0)]\n",
    "\n",
    "my_submission_final.to_csv(save_path / result_file_name, index_label='pred_id')\n",
    "print (\"Submission Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
